{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import prompt_templates_summarization as pts\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_google_genai import (\n",
    "    ChatGoogleGenerativeAI,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from json.decoder import JSONDecodeError\n",
    "from google.generativeai.types import BlockedPromptException\n",
    "from google.generativeai.types.generation_types import StopCandidateException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"Italy\"\n",
    "path2SP = \"/Users/ctoruno/OneDrive - World Justice Project/EU Subnational\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GoogleAI_key = os.getenv(\"googleAI_API_key\")\n",
    "OpenAI_key   = os.getenv(\"openai_key\")\n",
    "os.environ['GOOGLE_API_KEY'] = GoogleAI_key\n",
    "os.environ['OPENAI_API_KEY'] = OpenAI_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherFiles(country, p):\n",
    "    \"\"\"\n",
    "    This function takes a country as input and returns a list with all the news articles associated to that specific pillar.\n",
    "    \"\"\"\n",
    "    data_path = f\"{path2SP}/EU-S Data/Automated Qualitative Checks/Data/data-summarization/{country}/pillar_{p}\"\n",
    "    sets = [pd.read_parquet(f\"{data_path}/{x}\") for x in os.listdir(data_path)]\n",
    "    pillar_data = pd.concat(sets)\n",
    "    pillar_data[\"associated_pillar\"] = f\"Pillar {p}\"\n",
    "    print(f\"Pillar {p}: {len(pillar_data)} articles\")\n",
    "\n",
    "    return pillar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = [gatherFiles(country, p) for p in range(1,9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving gathered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vars = [\n",
    "    \"id\", \"link\", \"domain_url\", \"published_date\",\n",
    "    \"title_trans\", \"description_trans\", \"content_trans\", \"summary\", \"impact_score\",\n",
    "    \"pillar_1\", \"pillar_2\", \"pillar_3\", \"pillar_4\", \"pillar_5\", \"pillar_6\", \"pillar_7\", \"pillar_8\",\n",
    "    \"associated_pillar\"\n",
    "]\n",
    "master = pd.concat(country_data).loc[:,target_vars]\n",
    "master[\"published_date\"] = pd.to_datetime(master['published_date'])\n",
    "master.sort_values(\n",
    "    by = \"published_date\",\n",
    "    inplace = True,\n",
    "    ignore_index = True\n",
    ")\n",
    "master.to_parquet(f\"{path2SP}/EU-S Data/Automated Qualitative Checks/Data/data-summarization/{country}/{country}_master.parquet.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"\n",
    "        Parse the output of an LLM call to a valid JSON format.\n",
    "        \"\"\"\n",
    "        return json.loads(text.replace('```json', '').replace('```', ''), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pillar(summaries, pillar):\n",
    "    \"\"\"\n",
    "    This function takes a list of summaries and it sends a call to Google's Gemini asking for a list of\n",
    "    important events to take into account for a specific pillar of the Rule of Law.\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = str(pillar)\n",
    "\n",
    "    # Setting up the Prompt Template\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "                    (\"human\", pts.pillar_summary_prompt)\n",
    "                ])\n",
    "\n",
    "    # Defining our chain\n",
    "    chain_gemini = chat_prompt | ChatGoogleGenerativeAI(model = \"gemini-pro\",\n",
    "                                                        temperature = 0.25, \n",
    "                                                        safety_settings = safety_settings,\n",
    "                                                        convert_system_message_to_human = True) | JSONOutputParser()\n",
    "    \n",
    "    try: \n",
    "        llm_response = chain_gemini.invoke({\n",
    "            \"summaries\"      : summaries,\n",
    "            \"pillar_name\"    : pts.pillar_names[idx],\n",
    "            \"pillar_bullets\" : pts.pillar_bullets[idx]\n",
    "        })\n",
    "        status = True\n",
    "        time.sleep(1)   # We need to slow down the calls. given that the Gemini API has a limit of 60 calls per second\n",
    "\n",
    "    # The API can still block some of our prompts due to undefined reasons. Sadly, we can't do anything about it, so we\n",
    "    # predefine the outcome    \n",
    "    except (BlockedPromptException, StopCandidateException):\n",
    "        print(\"Prompt BLOCKED\")\n",
    "        status = False\n",
    "    \n",
    "    except JSONDecodeError:\n",
    "        print(\"Decode error... trying again...\")\n",
    "        try: \n",
    "            llm_response = chain_gemini.invoke({\n",
    "                \"summaries\"      : summaries,\n",
    "                \"pillar_name\"    : pts.pillar_names[idx],\n",
    "                \"pillar_bullets\" : pts.pillar_bullets[idx]\n",
    "            })\n",
    "            status = True\n",
    "            time.sleep(1)\n",
    "\n",
    "        except JSONDecodeError:\n",
    "            print(\"Decode error... trying again...\")\n",
    "            try: \n",
    "                llm_response = chain_gemini.invoke({\n",
    "                    \"summaries\"      : summaries,\n",
    "                    \"pillar_name\"    : pts.pillar_names[idx],\n",
    "                    \"pillar_bullets\" : pts.pillar_bullets[idx]\n",
    "                })\n",
    "                status = True\n",
    "                time.sleep(1)\n",
    "\n",
    "            except JSONDecodeError:\n",
    "                print(\"Failed. Skipping...\")\n",
    "                status = False\n",
    "\n",
    "    # We use the STATUS variable to throw an outcome to our call depending if our prompt was blocked or not\n",
    "    if status == True:\n",
    "        outcome = [llm_response[\"list_of_events\"]]\n",
    "\n",
    "    else:\n",
    "        outcome = \"Skipped list\"\n",
    "\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pillar_GPT(summaries, pillar):\n",
    "    \"\"\"\n",
    "    This function takes a list of summaries and it sends a call to OpenAI's GPT asking for a list of\n",
    "    important events to take into account for a specific pillar of the Rule of Law.\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = str(pillar)\n",
    "\n",
    "    # Setting up the Prompt Template\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "                    (\"system\", pts.pillar_summary_context),\n",
    "                    (\"human\", pts.pillar_summary_instructions)\n",
    "                ])\n",
    "\n",
    "    # Defining our chain\n",
    "    chain_gpt = chat_prompt | ChatOpenAI(model_name=\"gpt-4o\", temperature=0.2) | JSONOutputParser()\n",
    "    \n",
    "    try: \n",
    "        llm_response = chain_gpt.invoke({\n",
    "            \"summaries\"      : summaries,\n",
    "            \"pillar_name\"    : pts.pillar_names[idx],\n",
    "            \"pillar_bullets\" : pts.pillar_bullets[idx]\n",
    "        })\n",
    "        status = True\n",
    "    \n",
    "    except JSONDecodeError:\n",
    "        print(\"Decode error... trying again...\")\n",
    "        try: \n",
    "            llm_response = chain_gpt.invoke({\n",
    "                \"summaries\"      : summaries,\n",
    "                \"pillar_name\"    : pts.pillar_names[idx],\n",
    "                \"pillar_bullets\" : pts.pillar_bullets[idx]\n",
    "            })\n",
    "            status = True\n",
    "\n",
    "        except JSONDecodeError:\n",
    "            print(\"Decode error... trying again...\")\n",
    "            try: \n",
    "                llm_response = chain_gpt.invoke({\n",
    "                    \"summaries\"      : summaries,\n",
    "                    \"pillar_name\"    : pts.pillar_names[idx],\n",
    "                    \"pillar_bullets\" : pts.pillar_bullets[idx]\n",
    "                })\n",
    "                status = True\n",
    "\n",
    "            except JSONDecodeError:\n",
    "                print(\"Failed. Skipping...\")\n",
    "                status = False\n",
    "\n",
    "    # We use the STATUS variable to throw an outcome to our call depending if our prompt was blocked or not\n",
    "    if status == True:\n",
    "        outcome = [llm_response[\"list_of_events\"]]\n",
    "\n",
    "    else:\n",
    "        outcome = \"Skipped list\"\n",
    "\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_summaries(summaries, llm = \"GPT\"):\n",
    "    \"\"\"\n",
    "    This function takes a list of summaries and provides a text compiling all of them but taking into account the \n",
    "    token limit for using the Gemini Pro 1.0 API\n",
    "    \"\"\"\n",
    "\n",
    "    idx = 0\n",
    "    segments = [[]]\n",
    "    total_count = 0\n",
    "\n",
    "    if llm == \"GPT\":\n",
    "        tkn_limit = 98500\n",
    "    if llm == \"Gemini\":\n",
    "        tkn_limit = 22500\n",
    "\n",
    "    for text in summaries:\n",
    "        text_length = len(text.split())\n",
    "\n",
    "        if total_count + text_length < tkn_limit:\n",
    "            segments[idx].append(text)\n",
    "            total_count = total_count + text_length\n",
    "        else:\n",
    "            segments.append([])\n",
    "            idx = idx + 1\n",
    "            total_count = 0\n",
    "            segments[idx].append(text)\n",
    "            total_count = total_count + text_length\n",
    "\n",
    "    outcome = [\"- \"+\"\\n- \".join(segment) for segment in segments]\n",
    "\n",
    "    return outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {}\n",
    "for p in range(1,9):\n",
    "    print(f\"PILLAR {p}\")\n",
    "    article_list   = country_data[p-1][\"summary\"].to_list()\n",
    "    limited_chunks = split_summaries(article_list)\n",
    "    print(f\"Total number of chunks: {len(limited_chunks)}\")\n",
    "    bullet_points  = [summarize_pillar_GPT(x, p) for x in limited_chunks]\n",
    "    pillar_summ    = \"/n\".join([item for sublist_1 in bullet_points for sublist_2 in sublist_1 for item in sublist_2])\n",
    "    summaries.update({f\"Pillar {p}\" : pillar_summ})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving summaries as JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{path2SP}/EU-S Data/Automated Qualitative Checks/Data/data-summarization/0_Overview/{country}_overviewSummaries.json\"\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(summaries, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
